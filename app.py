import os

from dotenv import load_dotenv
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi_cors import CORS
from langchain_pinecone import PineconeVectorStore
from langchain_upstage import ChatUpstage
from langchain_openai import ChatOpenAI
from langchain_upstage import UpstageEmbeddings
from pinecone import Pinecone, ServerlessSpec
from pydantic import BaseModel
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder

app = FastAPI()

# í™˜ê²½ ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸°
load_dotenv()

# upstage
chat_upstage = ChatUpstage(
    model='solar-pro',
    temperature=0.1,
    top_p=0.8
)
chat_openai = ChatOpenAI(
    model_name="gpt-4o-mini",
    temperature=0.1,
    top_p=1,
    max_tokens=256,
)
embedding_upstage = UpstageEmbeddings(model='solar-embedding-1-large')

pinecone_api_key = os.environ.get('PINECONE_API_KEY')
pc = Pinecone(api_key=pinecone_api_key)
index_name = 'house-lease'

# create new index
if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=4096,
        metric="cosine",
        spec=ServerlessSpec(cloud='aws', region='us-east-1')
    )

pinecone_vectorstore = PineconeVectorStore(index=pc.Index(index_name), embedding=embedding_upstage)
pinecone_retriever = pinecone_vectorstore.as_retriever(
    search_type='similarity',
    search_kwargs={'k': 15}
)

# ReRanker: CrossEncoder
model = HuggingFaceCrossEncoder(
    model_name="BAAI/bge-reranker-base",
)

compressor = CrossEncoderReranker(model=model, top_n=5)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor, base_retriever=pinecone_retriever
)

origins = [
    "https://ai-solomon-project.vercel.app",  # Vercel ë„ë©”ì¸
    "https://*.vercel.app",              # Vercel ì„œë¸Œë„ë©”ì¸ í—ˆìš©
    "http://localhost:3000",             # ë¡œì»¬ ê°œë°œìš©
]


app.add_middleware(
    CORS,
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=['*'],
    allow_headers=['*'],
)

class MessageRequest(BaseModel):
    messages: str

class ChatMessage(BaseModel):
    role: str
    content: str


@app.get('/')
async def root():
    return {'reply': 'local host test'}

@app.post('/chat')
async def chat(req: MessageRequest):

    user_messages = req.messages
    print(f'User messages: {user_messages}')

    prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
            ë„ˆëŠ” ê³„ì•½ ì¡°í•­ì— ê´€í•œ ë²•ë¥  ì •ë³´ë¥¼ ì¼ë°˜ì¸ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ëŠ” ë²•ë¥  ì „ë¬¸ê°€ì•¼.
            í•­ìƒ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ê³  ì‰½ê²Œ ì„¤ëª…í•´ ì¤˜.
            
            ì‚¬ìš©ìê°€ ë¶€ë™ì‚° ê´€ë ¨ ì§ˆë¬¸ì„ í•˜ì§€ ì•ŠëŠ” ê²½ìš°(ì˜ˆ: 'happy', 'ë­ ë¨¹ì§€' ë“±)
            ë¶€ë™ì‚°ê³¼ ê´€ë ¨ ìˆëŠ” ì§ˆë¬¸ì„ í•´ë‹¬ë¼ê³  í•´.

            ë°˜ë“œì‹œ ì•„ë˜ [context] ë‚´ìš©ì„ ì°¸ê³ í•´ì„œë§Œ ë‹µë³€í•´ì•¼ í•´.
            ë¨¼ì € [context] ì•ˆì— ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ì¡°í•­ì´ë‚˜ ì‚¬ë¡€ê°€ ìˆëŠ”ì§€ ë°˜ë“œì‹œ í™•ì¸ì„ í•´.
            ë§Œì•½ ê´€ë ¨ ì¡°í•­(í‚¤ì›Œë“œ, íŒë¡€ ë“±)ì´ ì—†ìœ¼ë©´ 'ê´€ë ¨ ê·¼ê±° ì—†ìŒ'ì´ë¼ê³  íŒë‹¨í•´.
            ê´€ë ¨ ê·¼ê±°ê°€ ì—†ì„ ê²½ìš° í•´ë‹¹ ìƒí™©ì— ëŒ€í•´ ëª…ì‹œì  ê·œì •ì´ ë³´ì´ì§€ ì•ŠëŠ”ë‹¤ê³  ë°íŒ ë’¤ ì¼ë°˜ì ì¸ ê³„ì•½ ì‚¬í•­ ì›ì¹™ì„ ì„¤ëª…í•´ ì¤˜.
            (ì˜ˆ: ì„ì°¨ì¸ì˜ ì¤‘ë„ í•´ì§€, ì •ë‹¹í•œ ì‚¬ìœ  ê¸°ì¤€ ë“±)
            ì£¼ì˜! [context]ì— ëª…ì‹œë˜ì§€ ì•Šì€ ì‚¬ì‹¤ì„ ì„ì˜ë¡œ ì¶”ì •í•˜ê±°ë‚˜ ì°½ì‘í•˜ì§€ ë§ˆ.
            ì‚¬ìš©ìì˜ ìƒí™©ì— ëŒ€í•´ ìœ ë¦¬í•˜ê²Œ ì¶”ì •í•˜ì§€ ë§ê³  ì •í™•í•œ ì‚¬ì‹¤ë§Œ ë§í•´.

            íŠ¹íˆ ê°œì¸ ì‚¬ì •ì— ë”°ë¼ ê³„ì•½ì´ ë°”ë€ŒëŠ” ê²½ìš°ëŠ” ê´€ë ¨ ì¡°í•­, íŒë¡€ê°€ ëª…ì‹œë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´
            í•´ë‹¹ ìƒí™©ì— ëŒ€í•´ ë³¸ ê³„ì•½, íŒë¡€ì—ì„œ í™•ì¸ë˜ì§€ ì•ŠëŠ”ë‹¤ê³  ë¨¼ì € ë°íŒ í›„ ì¼ë°˜ ì›ì¹™ì„ ì„¤ëª…í•´ ì¤˜.
            [context]ì— ê´€ë ¨ ì¡°í•­ì´ ìˆì„ ë•Œë§Œ ê·¸ ì¡°í•­ì„ ê·¼ê±°ë¡œ íŒë‹¨í•´ ì¤˜.

            ì‚¬ìš©ìê°€ ê³„ì•½ ë¬¸êµ¬ë‚˜ ì‚¬ë¡€ì— ëŒ€í•´ ì§ˆë¬¸í•˜ë©´ ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œë§Œ ëŒ€ë‹µí•´ ì¤˜:

            ğŸ“˜ [ë²•ì  í•´ì„]:
            ğŸ—£ [ì‰½ê²Œ ë§í•˜ë©´]:
            âš ï¸ [ì£¼ì˜í•  ì ]:

            ê° ë¸”ë¡ì€ 3~4ì¤„, 250ì ì •ë„ë¡œ ì œí•œí• ê²Œ.
            ì „ë¬¸ ìš©ì–´ëŠ” ê´„í˜¸ë¡œ ê°„ë‹¨íˆ í•´ì„¤ì„ ë‹¬ì•„ ì¤˜,
            [ì‰½ê²Œ ë§í•˜ë©´] ë¸”ë¡ì— í•„ìš”í•  ê²½ìš° 'ì˜ˆ: ...' í˜•íƒœì˜ ì§§ì€ ì‚¬ë¡€ë¥¼ í•œ ì¤„ ì œê³µí•´ ì¤˜.

            [context]
            {context}
            """,
        ),
        ("human", "ì„ì°¨ì¸ì€ ê³„ì•½ê¸°ê°„ ì¤‘ ì„ì˜ í•´ì§€ë¥¼ í•  ìˆ˜ ì—†ìœ¼ë©°, ì´ì— ìœ„ë°˜í•  ê²½ìš° ì”ì—¬ê¸°ê°„ ì›”ì„¸ë¥¼ ìœ„ì•½ê¸ˆìœ¼ë¡œ ì²­êµ¬í•  ìˆ˜ ìˆë‹¤."),
        ("ai", """
ğŸ“˜ [ë²•ì  í•´ì„]:
ì„ì°¨ì¸ì€ ê³„ì•½ ê¸°ê°„ì´ ëë‚˜ê¸° ì „ì—ëŠ” ì¤‘ë„ì— ê³„ì•½ì„ í•´ì§€í•  ìˆ˜ ì—†ìœ¼ë©°, ë§Œì•½ í•´ì§€í•  ê²½ìš° ë‚¨ì€ ê¸°ê°„ ë™ì•ˆì˜ ì›”ì„¸ë¥¼ ì „ë¶€ ë‚´ì•¼ í•œë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.

ğŸ—£ [ì‰½ê²Œ ë§í•˜ë©´]:
ì„¸ì…ìê°€ ê³„ì•½ ì¤‘ê°„ì— ë‚˜ê°€ë©´, ê·¸ë™ì•ˆì˜ ë‚¨ì€ ì›”ì„¸ë¥¼ ì „ë¶€ ë²Œê¸ˆì²˜ëŸ¼ ë‚´ì•¼ í•œë‹¤ëŠ” ë§ì´ì—ìš”.

âš ï¸ [ì£¼ì˜í•  ì ]:
ì´ ì¡°í•­ì€ ì‹¤ì œë¡œëŠ” â€˜ì •ë‹¹í•œ ì‚¬ìœ â€™ê°€ ìˆìœ¼ë©´ ì¤‘ë„ í•´ì§€ê°€ ê°€ëŠ¥í•˜ë¯€ë¡œ, ë¬´ì¡°ê±´ ìœ„ì•½ê¸ˆì„ ë‚´ì•¼ í•˜ëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. ë²•ë¥ ì ìœ¼ë¡œëŠ” íš¨ë ¥ì´ ì œí•œì ì¼ ìˆ˜ ìˆì–´ìš”.
        """),
        ("human", "{input}"),
    ])
    # Prompt chain
    chain = prompt | chat_openai | StrOutputParser()
    
    result_docs = compression_retriever.invoke(user_messages)
    context_text = "\n\n".join(d.page_content for d in result_docs)
    result = chain.invoke({'context': context_text, 'input': user_messages})

    print(f'Chain result: {result}')

    return {'reply': result}


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host='0.0.0.0', port=8000)